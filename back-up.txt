A tree predictor is a learning algorithm that can be employed for binary classification tasks. It has an advantage over algorithms such as the Nearest Neighbor, as it can handle both numerical and non-numerical features. 

It has a structure that resembles a tree, where each node is either a leaf (i.e., it has zero children) or an internal node (i.e., it has at least two children). Given \( \mathcal{X} = \mathcal{X}_1 \times \dots \times \mathcal{X}_D \), where \( \mathcal{X}_i \) represents the domain of the \( i \)-th feature, the tree \( T \) whose internal nodes are associated with tests and whose leaves are tagged with labels in \( \mathcal{Y} \) generates a classifier \( h_T : \mathcal{X} \to \mathcal{Y} \). 

A single-node tree is a classifier that assigns to any data point the most frequent label in the training set. The tree is then grown by replacing a leaf with an internal node and two new leaves. For each leaf \( \ell \), we define \( S_\ell = \{(\mathbf{x}_i, y_i) \in S : \mathbf{x}_i \text{ is routed to } \ell \} \), where \( S_\ell \) represents the subset of data points routed to leaf \( \ell \). We can further define two subsets of \( S_\ell \):
\[
S_\ell^+ = \{ (\mathbf{x}_i, y_i) \in S_\ell : y_i = 1 \}, \quad S_\ell^- = \{ (\mathbf{x}_i, y_i) \in S_\ell : y_i = -1 \},
\]
where \( N_\ell^+ = |S_\ell^+| \), \( N_\ell^- = |S_\ell^-| \), and \( N_\ell = |S_\ell| = N_\ell^+ + N_\ell^- \).

The core principle behind tree predictors is to divide the feature space into regions that are as homogeneous as possible with respect to the output label. This is achieved through a recursive process of splitting the data at each node, guided by a splitting criterion such as the Gini Index or entropy. Each internal node represents a decision based on a feature and its value, while the leaf nodes correspond to predictions or output labels.

A tree predictor operates by routing input data from the root to a specific leaf node based on feature values. The prediction for the input is then determined by the label or value associated with the reached leaf node.

\subsubsection*{n\_quantiles Parameter:}

The \texttt{n\_quantiles} parameter specifies the number of quantiles to consider when determining the best threshold for splitting continuous features. The default value is \texttt{None}, which results in all midpoints between unique values being considered as candidate thresholds. Otherwise:

\begin{itemize}
    \item If \texttt{n\_quantiles} is an integer, the continuous feature values are divided into that many quantiles, and the candidate thresholds are chosen as the boundaries between these quantiles.
\end{itemize}

By adjusting the \texttt{n\_quantiles} parameter, the granularity of threshold selection is controlled. Lower values of \texttt{n\_quantiles} reduce the number of candidate thresholds, speeding up the computation but potentially leading to less optimal splits. Conversely, higher values of \texttt{n\_quantiles}, or setting it to \texttt{None} (to consider all midpoints), increase the granularity of search, improving the likelihood of finding an optimal split but at the cost of additional computation time.

\subsubsection*{isolate\_one Parameter:}

The \texttt{isolate\_one} parameter determines how splits are made for categorical features. The default value is \texttt{False}, which orders all values alphabetically and splits them based on a threshold value, grouping all values lower than or equal to the threshold on one side and all other values on the other side. Otherwise:

\begin{itemize}
\item If \texttt{isolate\_one} is \texttt{True}, the algorithm isolates a single value from the feature values, creating a one-vs-rest split.
\end{itemize}

This parameter controls the granularity of splits for categorical features. Setting \texttt{True} creates more precise splits, which can capture finer patterns in the data but may increase the risk of overfitting. Conversely, setting \texttt{False} produces broader, more generalized splits, improving computational efficiency and reducing overfitting.