\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx} % To use \resizebox
\usepackage{array} % For custom column widths
\usepackage{calc} % To use \widthof

\usepackage{siunitx} % Formatting numbers in a table

\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
%\usepackage{placeins}

\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}

\author{Julia Maria Wdowinska}
\date{} % Remove date from the title

\begin{document}

\begin{titlepage}
    \centering
    \vfill
    {\scshape\Large University of Milan \par}
    \vspace{0.5cm}
    {\scshape\large Faculty of Political, Economic and Social Sciences \par}
    \vspace{3cm}
    {\huge
    \textbf{Binary Tree Classifier from Scratch for Mushroom Classification} \\
    \vspace{0.5cm}
    \large Final Project in the Subject Machine Learning \par}
    \vspace{2cm}
    {\large \textbf{Julia Maria Wdowinska} \par}
    \vspace{0.5cm}
    {\large Data Science for Economics \par}
    {\large I year\par}
    {\large Master’s Degree \par}
    {\large Matriculation Number:\ 43288A \par}
\vfill
\begin{center}
\begin{figure}[h!]\centering
 \includegraphics[keepaspectratio=true,scale=0.2]{logo} \\
\end{figure}
\end{center}
\vfill
    {\large \today \par}
    \vfill
\end{titlepage}

\tableofcontents
%\newpage

\section{Introduction}

\section{Tree Classifier Implementation}

Tree predictors are fundamental tools in machine learning, widely applied to classification and regression tasks. They represent a hierarchy of decision rules, where data points are recursively split into subsets based on feature values. The main advantage of tree predictors is their ability to handle both numerical and categorical features. Tree predictors are also straightforward, making them a popular choice when interpretability is a priority.

In this study, a complete binary tree classifier - where each internal node has exactly two children - has been implemented in Python. A detailed description of the classes and methods created is provided below.

\subsection{TreeNode Class}

The \texttt{TreeNode} class represents a single node in a binary tree classifier. Each node can either be an internal node or a leaf node. Internal nodes split the data based on a specific feature and threshold, while leaf nodes store the predicted label. The attributes and methods of this class are designed to support the recursive structure of the tree classifier.

\subsubsection*{Attributes:}
\begin{itemize}
    \item \texttt{feature\_index} (int or None): The feature index used for splitting the data at this node.
    \item \texttt{threshold\_value} (float or None): The threshold value that determines how the data is split at this node.
    \item \texttt{left\_child} (TreeNode or None): The left child node.
    \item \texttt{right\_child} (TreeNode or None): The right child node.
    \item \texttt{left\_ratio} (float or None): The ratio of samples that go to the left child. This value is particularly useful for handling missing values and calculating probabilities.
    \item \texttt{leaf\_value} (int or None): The predicted label associated with the leaf node.
\end{itemize}

\noindent If the node is a leaf node, then \texttt{feature\_index}, \texttt{threshold\_value}, \texttt{left\_child}, \texttt{right\_child}, and \texttt{left\_ratio} are \texttt{None}, while \texttt{leaf\_value} is an integer. If the node is not a leaf node, then only \texttt{leaf\_value} is \texttt{None}.

\subsubsection*{Methods:}
\begin{itemize}
    \item \texttt{is\_leaf()}: This method checks whether the current node is a leaf node. It returns \texttt{True} if the node has a \texttt{leaf\_value}, and \texttt{False} otherwise.
\end{itemize}

\subsection{DecisionTreeClassifier Class}

The \texttt{DecisionTreeClassifier} class implements a decision tree for binary classification. It recursively splits the data based on specific features and thresholds, creating a tree structure that can be used for predicting labels. The attributes and methods of this class are designed to support model training, hyperparameter tuning, and prediction.

\subsubsection*{Attributes:}
\begin{itemize}
    \item \texttt{min\_samples\_split} (int): The minimum number of samples required to split a node.
    \item \texttt{max\_depth} (int or None): The maximum depth of the tree. If set to \texttt{None}, the tree expands until all nodes are pure, contain fewer than \texttt{min\_samples\_split} samples, or further splitting results in an information gain below \texttt{min\_information\_gain}.
    \item \texttt{n\_features} (int, float, or str): The number of features to consider when identifying the best split. This can be specified as an integer, a float, or one of the following strings:\ \texttt{`sqrt'} or \texttt{`log2'}.
    \item \texttt{criterion} (str): The function to measure the quality of a split. Options are:\ \texttt{`gini'}, \texttt{`scaled\_entropy'}, and \texttt{`square\_root'}.
    \item \texttt{min\_information\_gain} (float): The minimum information gain required to perform a split.
    \item \texttt{n\_quantiles} (int or None): The number of quantiles to consider when determining the best threshold for continuous features. If set to \texttt{None}, the algorithm uses midpoints of unique values.
    \item \texttt{isolate\_one} (bool): Whether to isolate a single value for categorical features, creating a one-vs-rest split.
    \item \texttt{root} (TreeNode or None): The root node of the decision tree.
    \item \texttt{depth} (int): The final depth of the tree after it has been built.
\end{itemize}

\subsubsection*{min\_samples\_split Parameter:}

The \texttt{min\_samples\_split} parameter controls the minimum number of samples a node must contain to be eligible for splitting. If a node has fewer than \texttt{min\_samples\_split} samples, it becomes a leaf node, and no further splits are attempted.

A higher value for \texttt{min\_samples\_split} reduces the depth of the tree, making it less prone to capturing noise in the data. Conversely, a lower value allows the tree to grow deeper and potentially capture finer details, which can be beneficial for highly complex datasets but may increase the risk of overfitting. The default value is 2.

\subsubsection*{n\_features Parameter:}

The \texttt{n\_features} parameter specifies the number of features to consider when identifying the best split. The default value is \texttt{None}, which results in all features in the dataset being considered. Otherwise:

\begin{itemize}
    \item If \texttt{n\_features} is an integer, this specifies the exact number of features to consider. If the value exceeds the total number of features in the dataset, all features are considered instead.
    \item If \texttt{n\_features} is a float, it represents a fraction of the total number of features. The number of features to consider is calculated by multiplying this fraction by the total number of features and truncating the decimal part to obtain an integer. At least one feature is considered.
    \item If \texttt{n\_features} is a string, it can be either \texttt{`sqrt'} or \texttt{`log2'}, and the number of features is calculated as follows:
    \begin{itemize}
        \item \texttt{`sqrt'}: Sets the number of features to the square root of the total number of features, truncating the decimal part to obtain an integer. At least one feature is considered.
        \item \texttt{`log2'}: Sets the number of features to the base-2 logarithm of the total number of features, truncating the decimal part to obtain an integer. At least one feature is considered.
    \end{itemize}
\end{itemize}

This parameter allows the decision tree model to use a subset of features, which can help improve the model’s efficiency and performance, particularly when working with high-dimensional datasets.

\subsubsection*{criterion Parameter:}

The \texttt{criterion} parameter specifies the function used to measure the quality of a split in the decision tree. The available options are:

\begin{itemize}
    \item \texttt{`gini'} (the default): The Gini impurity is used, which is computed as:
    \[
    \text{Gini} = 2 \cdot p_0 \cdot (1 - p_0)
    \]
    where \( p_0 \) is the probability of class '0' within the node.
    
    \item \texttt{`scaled\_entropy'}: The scaled entropy is used. The entropy is scaled by halving the probabilities before applying the standard entropy formula:
    \[
    \text{Scaled Entropy} = -\sum_{i} \frac{p_i}{2} \cdot \log_2(p_i + \epsilon)
    \]
    where \( p_i \) is the probability of class \( i \), and \( \epsilon \) is a small constant to avoid taking the logarithm of zero.
    
    \item \texttt{`square\_root'}: The ``square root'' impurity is used, which is calculated as:
    \[
    \text{Square Root Impurity} = \sqrt{p_0 \cdot (1 - p_0)}
    \]
    where \( p_0 \) is the probability of class '0' within the node.
\end{itemize}

\subsubsection*{min\_information\_gain Parameter:}

The \texttt{min\_information\_gain} parameter specifies the minimum amount of information gain required to perform a split. Information gain measures the reduction in impurity after a split. It is computed as follows:
\[
    \text{Information Gain} = \text{Impurity Before Split} - \text{Weighted Impurity After Split}
\]
where the impurity is calculated using the selected \texttt{criterion}, such as Gini impurity, scaled entropy, or square root impurity. The weighted impurity after the split is calculated as:
\[
    \text{Weighted Impurity After Split} = \frac{L}{n} \cdot \text{Impurity of Left Child} + \frac{R}{n} \cdot \text{Impurity of Right Child}
\]
where:
\begin{itemize}
    \item \( L \) and \( R \) are the number of samples in the left and right child nodes, respectively.
    \item \( n \) is the total number of samples in the parent node.
\end{itemize}

The \texttt{min\_information\_gain} parameter accepts a float value that sets the threshold for the minimum information gain. If the calculated information gain from a potential split is less than this threshold, the split is not performed, and the node becomes a leaf node. The default value is 0.0.

\subsubsection*{n\_quantiles Parameter:}

The \texttt{n\_quantiles} parameter determines how candidate thresholds are chosen when splitting based on numerical features. If set to \texttt{None} (the default), all midpoints between unique values are considered. Otherwise:

\begin{itemize}
    \item If \texttt{n\_quantiles} is an integer, the values are divided into that many quantiles, and the candidate thresholds are the boundaries between these quantiles.
\end{itemize}

While lower values of \texttt{n\_quantiles} reduce the number of candidate thresholds, speeding up computation but potentially leading to suboptimal splits, higher values or setting it to \texttt{None} (to consider all midpoints) increase the search granularity, increasing the probability of finding an optimal split, but at the cost of additional compu- tation time.

\subsubsection*{isolate\_one Parameter:}

The \texttt{isolate\_one} parameter controls how splits are made when splitting based on categorical features. If set to \texttt{False} (the default), all data points with a feature value lower or equal (i.e., lower or equal alphabetically) to the threshold are assigned to the left child, while all other data points are assigned to the right child. Otherwise:

\begin{itemize}
\item If \texttt{isolate\_one} is set to \texttt{True}, the algorithm creates a one-vs-rest split, where all data points with a fea- ture value equal to the threshold go to the left child, while all other data points go to the right child.
\end{itemize}

This parameter affects the granularity of splits for categorical features. Setting \texttt{isolate\_one} to \texttt{True} results in more precise splits, capturing finer patterns in the data but potentially increasing the risk of overfitting. In contrast, setting it to \texttt{False} produces broader, more generalized splits, improving computational efficiency and helping reduce overfitting.

\subsubsection*{Private Methods:}
\begin{itemize}
    \item \texttt{\_build\_tree()}: Recursively builds the tree by splitting the data based on the best feature and threshold. It stops if any stopping condition is met, e.g., \texttt{max\_depth}.
\item \texttt{\_get\_most\_common\_label()}: This method finds and returns the most common label in a given array.
    \item \texttt{\_find\_best\_split()}: Finds the best feature and threshold for splitting the data.
    \item \texttt{\_calculate\_information\_gain()}: Computes the information gain from a potential split based on a selec- ted criterion.
    \item \texttt{\_split()}: Splits the data based on the selected feature and threshold.
    \item \texttt{\_gini\_impurity()}: Computes the Gini impurity for the given labels.
    \item \texttt{\_scaled\_entropy()}: Computes the scaled entropy for the given labels.
    \item \texttt{\_square\_root\_impurity()}: Computes the ``square root'' impurity for the given labels.
    \item \texttt{\_traverse\_tree()}: Traverses the tree for a single input sample and returns the predicted label.
\end{itemize}

\subsubsection*{Public Methods:}
\begin{itemize}
    \item \texttt{fit()}: Initializes the root node and builds the tree using the \texttt{\_build\_tree()} method.
    \item \texttt{predict()}: Predicts the labels for the given input samples by traversing the tree for each sample using the \texttt{\_traverse\_tree()} method.
\end{itemize}

\subsubsection*{\_build\_tree() Method:}

The \texttt{\_build\_tree()} method constructs a decision tree by starting at the root node and progressing recursively to the leaf nodes. At each node, it selects a random subset of features, as specified by the \texttt{n\_features} parameter, and employs the \texttt{\_find\_best\_split()} method to determine the optimal feature and threshold for splitting the data. Once the best split is found, the \texttt{\_split()} method is called to partition the data accordingly. The process is then repeated recursively on the resulting subsets to continue building the tree.

The recursion halts when a stopping condition is met, such as reaching the \texttt{max\_depth}, achieving pure nodes, having fewer than \texttt{min\_samples\_split} samples per node, or when subsequent splits yield information gains lower than \texttt{min\_information\_gain}. Upon termination, the method assigns the most frequent label among the samples at that node as the predicted label for that node.

\subsubsection*{\_find\_best\_split() Method:}

The \texttt{\_find\_best\_split()} method identifies the optimal split for a given node in the decision tree. It iterates through all features selected by the \texttt{\_build\_tree()} method and evaluates all candidate thresholds by calling the \texttt{\_calculate\_information\_gain()} method to determine the feature-threshold combination that maximizes information gain. The creation of candidate thresholds differs between numerical and categorical features:

\begin{itemize}
\item For numerical features, potential thresholds are determined based on the \texttt{n\_quantiles} parameter (as des- cribed above).
\item For categorical features, candidate thresholds consist of all unique values in the feature.
\end{itemize}

Any missing values are excluded when determining the candidate thresholds.

\subsubsection*{\_split() Method:}

The \texttt{\_split()} method partitions the data based on a specified feature and threshold. The partitioning strategy differs for numerical and categorical features:

\begin{itemize}
\item For numerical features, data points with a feature value lower than or equal to the threshold are assigned to the left child, while all other data points are assigned to the right child.
\item For categorical features, the partitioning depends on the \texttt{isolate\_one} parameter (as described above).
\end{itemize}

After the split, any data points with a missing feature value are randomly distributed between the left and right child. The probability of being assigned to each child is proportional to the number of data points assigned to that child during the split.

\subsubsection*{\_traverse\_tree() Method:}

The \texttt{\_traverse\_tree()} method traverses the decision tree to predict the label for a single instance. Starting at the root node, it follows the tree's decision rules until it reaches a leaf node. If the feature value at the current node is missing, the method randomly decides whether to move to the left or right child, based on the ratio of data points assigned to each child.

\section{Training and Validation Procedures}

Training a model on the training set and testing it on the test set is a fundamental practice in machine learning. The goal is to develop a model that generalizes well to new, unseen data, and not just memorizes the data seen during training (a phenomenon known as overfitting).

In this study, a set of custom functions has been implemented to facilitate robust model training, validation, and hyperparameter tuning.

\subsubsection*{Functions:}

\begin{itemize}
    \item \texttt{train\_test\_partition()}: 
    This function randomly partitions the data into training and testing sets. The proportion of data allocated to each set is specified by the user (the default is 20\% testing).
    
    \item \texttt{k\_fold\_partition()}:  
    This function divides the data into \( k \) folds for \( k \)-fold cross-validation. The number of folds is specified by the user (the default is 5).
    
    \item \texttt{k\_fold\_cv\_estimate()}:  
    This function uses the \texttt{k\_fold\_partition()} function to create \( k \) folds and trains the model \( k \) times, each time using \( k-1 \) folds for training and the remaining fold for testing. The average test error across all iterations (i.e., the cross-validation estimate) is returned.
    
    \item \texttt{hyperparameter\_tuning()}: 
        This function iterates through all parameter combinations generated by the \texttt{\_parameter\_combinations()} function, or a random subset if specified, and uses the \texttt{k\_fold\_cv\_estimate()} function to compute the cross-validation estimate for a model with these parameters. The lowest cross-validation estimate and the corresponding parameters are then returned.
        
    \item \texttt{k\_fold\_nested\_cv()}: 
    This function implements nested cross-validation. The \texttt{k\_fold\_partition()} function is used to create \( k \) folds and the model is trained \( k \) times. Each time, the \texttt{hyperparameter\_tuning()} function is invoked on \( k-1 \) folds and the model with the best parameters is tested on the remaining fold. Then the average test error across all iterations is returned.

    \item \texttt{\_parameter\_combinations()}: 
    This helper function generates all possible combinations of hyperparameters from a specified grid.

\item \texttt{accuracy\_metric()}: 
    This function computes the accuracy of the model’s predictions by calculating the proportion of correct predictions out of all predictions.

    \item \texttt{precision\_metric()}: 
    This function computes precision, defined as the ratio of true positive predictions to the total predicted positives.

    \item \texttt{recall\_metric()}: 
    This function computes recall, defined as the ratio of true positive predictions to the total actual positives.

    \item \texttt{f1\_metric()}: 
    This function computes the F1 score, the harmonic mean of precision and recall.

    \item \texttt{confusion\_matrix()}: 
    This function computes the confusion matrix, which summarizes the performance of a classification model by comparing the true labels against the predicted labels. The confusion matrix contains four elements:\ true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).
\end{itemize}

\section{Dataset Description}

The \href{https://archive.ics.uci.edu/dataset/848/secondary+mushroom+dataset}{dataset} used in this study is a simulated version inspired by the \href{https://archive.ics.uci.edu/dataset/73/mushroom}{Mushroom Data Set}. It contains 61,069 hypothetical mushrooms, each described by 20 features and classified as either definitely edible or definitely poisonous/of unknown edibility. The class distribution is balanced, with 27,181 mushrooms (44.51\%) classified as edible and 33,888 (55.49\%) as poisonous, ensuring no class is overrepresented in subsequent analyses.

During preprocessing, 146 duplicate rows (0.24\%) were identified and removed to avoid undue weighting of observations. This step reduced the dataset to 60,923 mushrooms, consisting of 27,181 (44.62\%) edible and 33,742 (55.38\%) poisonous instances. Additionally, nine variables were found to contain missing values, as summarized in Table \ref{tab:missing_values_before}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Missing Values Count and Percentage}
\label{tab:missing_values_before}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{does-bruise-or-bleed}}
S[table-format=5]
S[table-format=2.2]
}
\hline
& \multicolumn{2}{c}{\textbf{Missing Values}} \\ \cline{2-3}
\textbf{Variable} & \textbf{Count} & \textbf{Percentage} \\ \hline
cap-surface & 14120 & 23.12 \\ \hline
gill-attachment & 9855 & 16.14 \\ \hline
gill-spacing & 25062 & 41.04 \\ \hline
stem-root & 51536 & 84.39 \\ \hline
stem-surface & 38122 & 62.42 \\ \hline
veil-type & 57746 & 94.56 \\ \hline
veil-color & 53510 & 87.62 \\ \hline
ring-type & 2471 & 4.05 \\ \hline
spore-print-color & 54597 & 89.40 \\ \hline
\end{tabular}
\end{table}

Upon examining the possible values for each variable, some unexpected values not defined by the dataset's author were observed, such as ``d'' for cap-surface and ``f'' for stem-root. A comprehensive overview of all 21 variables, their types, and possible values is provided in Table \ref{tab:mushroom_variables}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Mushroom Dataset Variables}
\label{tab:mushroom_variables}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{does-bruise-or-bleed}}
>{\raggedright\arraybackslash}p{\widthof{categorical}}
>{\raggedright\arraybackslash}p{\widthof{a (adnate), x (adnexed), d (decurrent), e (free), s (sinuate), p (pores), f (none)}}
}
\hline
\textbf{Variable} & \textbf{Type} & \textbf{Possible Values} \\ \hline
class & categorical & e (edible), p (poisonous/of unknown edibility) \\ \hline
cap-diameter & numerical & float number in cm \\ \hline
cap-shape & categorical & b (bell), c (conical), x (convex), f (flat), s (sunken), p (spherical), o (others) \\ \hline
cap-surface & categorical & i (fibrous), g (grooves), y (scaly), s (smooth), h (shiny), l (leathery), k (silky), t (sticky), w (wrinkled), e (fleshy), d (not specified by the author) \\ \hline
cap-color & categorical & n (brown), b (buff), g (gray), r (green), p (pink), u (purple), e (red), w (white), y (yellow), l (blue), o (orange), k (black) \\ \hline
does-bruise-or-bleed & categorical & t (bruises or bleeding), f (no) \\ \hline
gill-attachment & categorical & a (adnate), x (adnexed), d (decurrent), e (free), s (sinuate), p (pores), f (none) \\ \hline
gill-spacing & categorical & c (close), d (distant), f (none) \\ \hline
gill-color & categorical & n (brown), b (buff), g (gray), r (green), p (pink), u (purple), e (red), w (white), y (yellow), o (orange), k (black), f (none) \\ \hline
stem-height & numerical & float number in cm \\ \hline
stem-width & numerical & float number in mm \\ \hline
stem-root & categorical & b (bulbous), s (swollen), c (club), r (rooted), f (not specified by the author) \\ \hline
stem-surface & categorical & i (fibrous), y (scaly), s (smooth), h (shiny), k (silky), t (sticky), f (none), g (not specified by the author) \\ \hline
stem-color & categorical & n (brown), b (buff), g (gray), r (green), p (pink), u (purple), e (red), w (white), y (yellow), l (blue), o (orange), k (black), f (none) \\ \hline
veil-type & categorical & u (universal) \\ \hline
veil-color & categorical & n (brown), u (purple), e (red), w (white), y (yellow), k (black) \\ \hline
has-ring & categorical & t (ring), f (none) \\ \hline
ring-type & categorical & e (evanescent), r (flaring), g (grooved), l (large), p (pendant), z (zone), m (movable), f (none) \\ \hline
spore-print-color & categorical & n (brown), g (gray), r (green), p (pink), u (purple), w (white), k (black) \\ \hline
habitat & categorical & g (grasses), l (leaves), m (meadows), p (paths), h (heaths), u (urban), w (waste), d (woods) \\ \hline
season & categorical & s (spring), u (summer), a (autumn), w (winter) \\ \hline
\end{tabular}
\end{table}

Notably, several gill- and stem-related variables include ``f'' as a possible value, signifying ``none''. This raised a question about whether ``f'' represents a valid value or missing data. Analysis revealed that if any gill-related variable has a value of ``f'', all other gill-related variables for that mushroom also have the value ``f''. A similar pattern was found for stem-related variables. Specifically, 3,414 mushrooms lacked gill-related information, and 915 lacked stem-related information. These 915 mushrooms also had stem-height and stem-width equal to 0, confirming the consistency of this pattern.

Further investigation, informed by domain knowledge, clarified that some mushrooms naturally lack gills or stems. This observation suggests that the ``f'' values reflect genuine biological characteristics rather than missing data. Consequently, these values were treated as valid in subsequent analyses.

Lastly, the veil-type variable was examined for its potential utility in predictive modeling. This variable only takes the value ``u'', but it was missing for the majority (94.56\%) of observations. Additionally, when veil-type was missing, both classes (edible and poisonous) were represented, indicating that the missingness did not provide meaningful predictive information for classification. Given its lack of variability and predictive utility, the veil-type variable was excluded from further analysis.

\section{Model Training and Evaluation}

Since the implemented DecisionTreeClassifier class is able to handle missing values, no columns or rows were dropped from the dataset. In order to find the best hyperparameters, the nested cross-validation, with 5 folds for both outer and inner loop, was performed using the following parameter grid:

parameter_grid = {
    'min_samples_split': [2, 5, 10, 20],
    'max_depth': [5, 10, 15, 20, None],
    'n_features': ["log2", "sqrt", None],
    'criterion': ["gini", "scaled_entropy", "square_root"],
    'min_information_gain': [0.0, 0.01, 0.05, 0.1],
    'n_quantiles': [5, 10, 20],
    'isolate_one': [True, False]
}
    
Since evaluating all possible parameter combinations would mean evaluating 4,320 combinations, it was decided to select at random 50 parameter combinations to evaluate. The results were the following:

Mean Test Error: 0.0094
Minimum Test Error: 0.0084
Best Model Parameters (corresponding to Minimum Test Error):
  min_samples_split: 10
  max_depth: 20
  n_features: None
  criterion: square_root
  min_information_gain: 0.0
  n_quantiles: 10
  isolate_one: False

Then, in order to evaluate the obtained decision tree model, the dataset was split into training and test set (20\% test set), and the decision tree with the best parameters (obtained in the nested cross-validation process) was trained on the training part and tested on the test part. The following results were obtained.
    
Accuracy: 0.9914
Precision: 0.9935
Recall: 0.9910
F1 Score: 0.9923
Confusion Matrix:
TP: 6745, TN: 5335, FP: 44, FN: 61

\end{document}