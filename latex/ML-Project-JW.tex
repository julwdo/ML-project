\documentclass{article}
\usepackage{geometry}
\usepackage{graphicx} % To use \resizebox
\usepackage{array} % For custom column widths
\usepackage{calc} % To use \widthof

\usepackage{siunitx} % Formatting numbers in a table

\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
%\usepackage{placeins}

\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}

\author{Julia Maria Wdowinska}
\date{} % Remove date from the title

\begin{document}

\begin{titlepage}
    \centering
    \vfill
    {\scshape\Large University of Milan \par}
    \vspace{0.5cm}
    {\scshape\large Faculty of Political, Economic and Social Sciences \par}
    \vspace{3cm}
    {\huge
    \textbf{Binary Tree Classifier from Scratch for Mushroom Classification} \\
    \vspace{0.5cm}
    \large Final Project in the Subject Machine Learning \par}
    \vspace{2cm}
    {\large \textbf{Julia Maria Wdowinska} \par}
    \vspace{0.5cm}
    {\large Data Science for Economics \par}
    {\large I year\par}
    {\large Master’s Degree \par}
    {\large Matriculation Number:\ 43288A \par}
\vfill
\begin{center}
\begin{figure}[h!]\centering
 \includegraphics[keepaspectratio=true,scale=0.2]{logo} \\
\end{figure}
\end{center}
\vfill
    {\large \today \par}
    \vfill
\end{titlepage}

\tableofcontents
%\newpage

\section{Introduction}

\section{Tree Classifier Implementation}

Tree predictors are fundamental tools in machine learning, widely applied to classification and regression tasks. They represent a hierarchy of decision rules, where data points are recursively split into subsets based on feature values. The main advantage of tree predictors is their ability to handle both numerical and categorical features. Tree predictors are also straightforward, making them a popular choice when interpretability is a priority.

In this study, a complete binary tree classifier - where each internal node has exactly two children - has been implemented in Python. A detailed description of the classes and methods created is provided below.

\subsection{TreeNode Class}

The \texttt{TreeNode} class represents a single node in a binary tree classifier. Each node can either be an internal node or a leaf node. Internal nodes split the data based on a specific feature and threshold, while leaf nodes store the predicted label. The attributes and methods of this class are designed to support the recursive structure of the tree classifier.

\subsubsection*{Attributes:}
\begin{itemize}
    \item \texttt{feature\_index} (int or None): The feature index used for splitting the data at this node.
    \item \texttt{threshold\_value} (float or None): The threshold value that determines how the data is split at this node.
    \item \texttt{left\_child} (TreeNode or None): The left child node.
    \item \texttt{right\_child} (TreeNode or None): The right child node.
    \item \texttt{left\_ratio} (float or None): The ratio of samples that go to the left child. This value is particularly useful for handling missing values and calculating probabilities.
    \item \texttt{leaf\_value} (int or None): The predicted label associated with the leaf node.
\end{itemize}

\noindent If the node is a leaf node, then \texttt{feature\_index}, \texttt{threshold\_value}, \texttt{left\_child}, \texttt{right\_child}, and \texttt{left\_ratio} are \texttt{None}, while \texttt{leaf\_value} is an integer. If the node is not a leaf node, then only \texttt{leaf\_value} is \texttt{None}.

\subsubsection*{Methods:}
\begin{itemize}
    \item \texttt{is\_leaf()}: This method checks whether the current node is a leaf node. It returns \texttt{True} if the node has a \texttt{leaf\_value}, and \texttt{False} otherwise.
\end{itemize}

\subsection{DecisionTreeClassifier Class}

The \texttt{DecisionTreeClassifier} class implements a decision tree for binary classification. It recursively splits the data based on specific features and thresholds, creating a tree structure that can be used for predicting labels. The attributes and methods of this class are designed to support model training, hyperparameter tuning, and prediction.

\subsubsection*{Attributes:}
\begin{itemize}
    \item \texttt{min\_samples\_split} (int): The minimum number of samples required to split a node.
    \item \texttt{max\_depth} (int or None): The maximum depth of the tree. If set to \texttt{None}, the tree expands until all nodes are pure, contain fewer than \texttt{min\_samples\_split} samples, or further splitting results in an information gain below \texttt{min\_information\_gain}.
    \item \texttt{n\_features} (int, float, or str): The number of features to consider when identifying the best split. This can be specified as an integer, a float, or one of the following strings:\ \texttt{`sqrt'} or \texttt{`log2'}.
    \item \texttt{criterion} (str): The function to measure the quality of a split. Options are:\ \texttt{`gini'}, \texttt{`scaled\_entropy'}, and \texttt{`square\_root'}.
    \item \texttt{min\_information\_gain} (float): The minimum information gain required to perform a split.
    \item \texttt{n\_quantiles} (int or None): The number of quantiles to consider when determining the best threshold for continuous features. If set to \texttt{None}, the algorithm uses midpoints of unique values.
    \item \texttt{isolate\_one} (bool): Whether to isolate a single value for categorical features, creating a one-vs-rest split.
    \item \texttt{root} (TreeNode or None): The root node of the decision tree.
    \item \texttt{depth} (int): The final depth of the tree after it has been built.
\end{itemize}

\subsubsection*{min\_samples\_split Parameter:}

The \texttt{min\_samples\_split} parameter controls the minimum number of samples a node must contain to be eligible for splitting. If a node has fewer than \texttt{min\_samples\_split} samples, it becomes a leaf node, and no further splits are attempted.

A higher value for \texttt{min\_samples\_split} reduces the depth of the tree, making it less prone to capturing noise in the data. Conversely, a lower value allows the tree to grow deeper and potentially capture finer details, which can be beneficial for highly complex datasets but may increase the risk of overfitting. The default value is 2.

\subsubsection*{n\_features Parameter:}

The \texttt{n\_features} parameter specifies the number of features to consider when identifying the best split. The default value is \texttt{None}, which results in all features in the dataset being considered. Otherwise:

\begin{itemize}
    \item If \texttt{n\_features} is an integer, this specifies the exact number of features to consider. If the value exceeds the total number of features in the dataset, all features are considered instead.
    \item If \texttt{n\_features} is a float, it represents a fraction of the total number of features. The number of features to consider is calculated by multiplying this fraction by the total number of features and truncating the decimal part to obtain an integer. At least one feature is considered.
    \item If \texttt{n\_features} is a string, it can be either \texttt{`sqrt'} or \texttt{`log2'}, and the number of features is calculated as follows:
    \begin{itemize}
        \item \texttt{`sqrt'}: Sets the number of features to the square root of the total number of features, truncating the decimal part to obtain an integer. At least one feature is considered.
        \item \texttt{`log2'}: Sets the number of features to the base-2 logarithm of the total number of features, truncating the decimal part to obtain an integer. At least one feature is considered.
    \end{itemize}
\end{itemize}

This parameter allows the decision tree model to use a subset of features, which can help improve the model’s efficiency and performance, particularly when working with high-dimensional datasets.

\subsubsection*{criterion Parameter:}

The \texttt{criterion} parameter specifies the function used to measure the quality of a split in the decision tree. The available options are:

\begin{itemize}
    \item \texttt{`gini'} (the default): The Gini impurity is used, which is computed as:
    \[
    \text{Gini} = 2 \cdot p_0 \cdot (1 - p_0)
    \]
    where \( p_0 \) is the probability of class '0' within the node.
    
    \item \texttt{`scaled\_entropy'}: The scaled entropy is used. The entropy is scaled by halving the probabilities before applying the standard entropy formula:
    \[
    \text{Scaled Entropy} = -\sum_{i} \frac{p_i}{2} \cdot \log_2(p_i + \epsilon)
    \]
    where \( p_i \) is the probability of class \( i \), and \( \epsilon \) is a small constant to avoid taking the logarithm of zero.
    
    \item \texttt{`square\_root'}: The ``square root'' impurity is used, which is calculated as:
    \[
    \text{Square Root Impurity} = \sqrt{p_0 \cdot (1 - p_0)}
    \]
    where \( p_0 \) is the probability of class '0' within the node.
\end{itemize}

\subsubsection*{min\_information\_gain Parameter:}

The \texttt{min\_information\_gain} parameter specifies the minimum amount of information gain required to perform a split. Information gain measures the reduction in impurity after a split. It is computed as follows:
\[
    \text{Information Gain} = \text{Impurity Before Split} - \text{Weighted Impurity After Split}
\]
where the impurity is calculated using the selected \texttt{criterion}, such as Gini impurity, scaled entropy, or square root impurity. The weighted impurity after the split is calculated as:
\[
    \text{Weighted Impurity After Split} = \frac{L}{n} \cdot \text{Impurity of Left Child} + \frac{R}{n} \cdot \text{Impurity of Right Child}
\]
where:
\begin{itemize}
    \item \( L \) and \( R \) are the number of samples in the left and right child nodes, respectively.
    \item \( n \) is the total number of samples in the parent node.
\end{itemize}

The \texttt{min\_information\_gain} parameter accepts a float value that sets the threshold for the minimum information gain. If the calculated information gain from a potential split is less than this threshold, the split is not performed, and the node becomes a leaf node. The default value is 0.0.

\subsubsection*{n\_quantiles Parameter:}

The \texttt{n\_quantiles} parameter determines how candidate thresholds are chosen when splitting based on numerical features. If set to \texttt{None} (the default), all midpoints between unique values are considered. Otherwise:

\begin{itemize}
    \item If \texttt{n\_quantiles} is an integer, the values are divided into that many quantiles, and the candidate thresholds are the boundaries between these quantiles.
\end{itemize}

While lower values of \texttt{n\_quantiles} reduce the number of candidate thresholds, speeding up computation but potentially leading to suboptimal splits, higher values or setting it to \texttt{None} (to consider all midpoints) increase the search granularity, increasing the probability of finding an optimal split, but at the cost of additional compu- tation time.

\subsubsection*{isolate\_one Parameter:}

The \texttt{isolate\_one} parameter controls how splits are made when splitting based on categorical features. If set to \texttt{False} (the default), all data points with a feature value lower or equal (i.e., lower or equal alphabetically) to the threshold are assigned to the left child, while all other data points are assigned to the right child. Otherwise:

\begin{itemize}
\item If \texttt{isolate\_one} is set to \texttt{True}, the algorithm creates a one-vs-rest split, where all data points with a fea- ture value equal to the threshold go to the left child, while all other data points go to the right child.
\end{itemize}

This parameter affects the granularity of splits for categorical features. Setting \texttt{isolate\_one} to \texttt{True} results in more precise splits, capturing finer patterns in the data but potentially increasing the risk of overfitting. In contrast, setting it to \texttt{False} produces broader, more generalized splits, improving computational efficiency and helping reduce overfitting.

\subsubsection*{Private Methods:}
\begin{itemize}
    \item \texttt{\_build\_tree()}: Recursively builds the tree by splitting the data based on the best feature and threshold. It stops if any stopping condition is met, e.g., \texttt{max\_depth}.
\item \texttt{\_get\_most\_common\_label()}: This method finds and returns the most common label in a given array.
    \item \texttt{\_find\_best\_split()}: Finds the best feature and threshold for splitting the data.
    \item \texttt{\_calculate\_information\_gain()}: Computes the information gain from a potential split based on a selec- ted criterion.
    \item \texttt{\_split()}: Splits the data based on the selected feature and threshold.
    \item \texttt{\_gini\_impurity()}: Computes the Gini impurity for the given labels.
    \item \texttt{\_scaled\_entropy()}: Computes the scaled entropy for the given labels.
    \item \texttt{\_square\_root\_impurity()}: Computes the ``square root'' impurity for the given labels.
    \item \texttt{\_traverse\_tree()}: Traverses the tree for a single input sample and returns the predicted label.
\end{itemize}

\subsubsection*{Public Methods:}
\begin{itemize}
    \item \texttt{fit()}: Initializes the root node and builds the tree using the \texttt{\_build\_tree()} method.
    \item \texttt{predict()}: Predicts the labels for the given input samples by traversing the tree for each sample using the \texttt{\_traverse\_tree()} method.
\end{itemize}

\subsubsection*{\_build\_tree() Method:}

The \texttt{\_build\_tree()} method constructs a decision tree by starting at the root node and progressing recursively to the leaf nodes. At each node, it selects a random subset of features, as specified by the \texttt{n\_features} parameter, and employs the \texttt{\_find\_best\_split()} method to determine the optimal feature and threshold for splitting the data. Once the best split is found, the \texttt{\_split()} method is called to partition the data accordingly. The process is then repeated recursively on the resulting subsets to continue building the tree.

The recursion halts when a stopping condition is met, such as reaching the \texttt{max\_depth}, achieving pure nodes, having fewer than \texttt{min\_samples\_split} samples per node, or when subsequent splits yield information gains lower than \texttt{min\_information\_gain}. Upon termination, the method assigns the most frequent label among the samples at that node as the predicted label for that node.

\subsubsection*{\_find\_best\_split() Method:}

The \texttt{\_find\_best\_split()} method identifies the optimal split for a given node in the decision tree. It iterates through all features selected by the \texttt{\_build\_tree()} method and evaluates all candidate thresholds by calling the \texttt{\_calculate\_information\_gain()} method to determine the feature-threshold combination that maximizes information gain. The creation of candidate thresholds differs between numerical and categorical features:

\begin{itemize}
\item For numerical features, potential thresholds are determined based on the \texttt{n\_quantiles} parameter (as des- cribed above).
\item For categorical features, candidate thresholds consist of all unique values in the feature.
\end{itemize}

Any missing values are excluded when determining the candidate thresholds.

\subsubsection*{\_split() Method:}

The \texttt{\_split()} method partitions the data based on a specified feature and threshold. The partitioning strategy differs for numerical and categorical features:

\begin{itemize}
\item For numerical features, data points with a feature value lower than or equal to the threshold are assigned to the left child, while all other data points are assigned to the right child.
\item For categorical features, the partitioning depends on the \texttt{isolate\_one} parameter (as described above).
\end{itemize}

After the split, any data points with a missing feature value are randomly distributed between the left and right child. The probability of being assigned to each child is proportional to the number of data points assigned to that child during the split.

\subsubsection*{\_traverse\_tree() Method:}

The \texttt{\_traverse\_tree()} method traverses the decision tree to predict the label for a single instance. Starting at the root node, it follows the tree's decision rules until it reaches a leaf node. If the feature value at the current node is missing, the method randomly decides whether to move to the left or right child, based on the ratio of data points assigned to each child.

\section{Training and Validation Procedures}

Training a model on the training set and testing it on the test set is a fundamental practice in machine learning. The goal is to develop a model that generalizes well to new, unseen data, and not just memorizes the data seen during training (a phenomenon known as overfitting).

In this study, a set of custom functions has been implemented to facilitate robust model training, validation, and hyperparameter tuning.

\subsubsection*{Functions:}

\begin{itemize}
    \item \texttt{train\_test\_partition()}: 
    This function randomly partitions the data into training and testing sets. The proportion of data allocated to each set is specified by the user (the default is 20\% testing).
    
    \item \texttt{k\_fold\_partition()}:  
    This function divides the data into \( k \) folds for \( k \)-fold cross-validation. The number of folds is specified by the user (the default is 5).
    
    \item \texttt{k\_fold\_cv\_estimate()}:  
    This function uses the \texttt{k\_fold\_partition()} function to create \( k \) folds and trains the model \( k \) times, each time using \( k-1 \) folds for training and the remaining fold for testing. The average test error across all iterations (i.e., the cross-validation estimate) is returned.
    
    \item \texttt{hyperparameter\_tuning()}: 
        This function iterates through all parameter combinations generated by the \texttt{\_parameter\_combinations()} function and uses the \texttt{k\_fold\_cv\_estimate()} function to compute the cross-validation estimate for a model with these parameters. The lowest cross-validation estimate and the corres- ponding parameters are then returned.
        
    \item \texttt{k\_fold\_nested\_cv()}: 
    This function implements nested cross-validation. The \texttt{k\_fold\_partition()} function is used to create \( k \) folds and the model is trained \( k \) times. Each time, the \texttt{hyperparameter\_tuning()} function is invoked on \( k-1 \) folds and the model with the best parameters is tested on the remaining fold. Then the average test error across all iterations is returned.

    \item \texttt{\_parameter\_combinations()}: 
    This helper function generates all possible combinations of hyperparameters from a specified grid.

    \item \texttt{accuracy\_metric()}: 
    This function computes the accuracy of the model’s predictions.
\end{itemize}

\section{Dataset Description}

The \href{https://archive.ics.uci.edu/dataset/848/secondary+mushroom+dataset}{dataset} used in this study is a simulated version inspired by the \href{https://archive.ics.uci.edu/dataset/73/mushroom}{Mushroom Data Set from J.\ Schlimmer}. It contains 61,069 hypothetical mushrooms, each described by 20 features and classified as either definitely edible or definitely poisonous/of unknown edibility. Table \ref{tab:mushroom_variables} presents all 21 variables included in the dataset.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Mushroom Dataset Variables}
\label{tab:mushroom_variables}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{does-bruise-or-bleed}}
>{\raggedright\arraybackslash}p{\widthof{categorical}}
>{\raggedright\arraybackslash}p{\widthof{a (adnate), x (adnexed), d (decurrent), e (free), s (sinuate), p (pores), f (none)}}
}
\hline
\textbf{Variable} & \textbf{Type} & \textbf{Possible Values} \\ \hline
class & categorical & e (edible), p (poisonous/of unknown edibility) \\ \hline
cap-diameter & numerical & float number in cm \\ \hline
cap-shape & categorical & b (bell), c (conical), x (convex), f (flat), s (sunken), p (spherical), o (others) \\ \hline
cap-surface & categorical & i (fibrous), g (grooves), y (scaly), s (smooth), h (shiny), l (leathery), k (silky), t (sticky), w (wrinkled), e (fleshy), d (not specified by the author) \\ \hline
cap-color & categorical & n (brown), b (buff), g (gray), r (green), p (pink), u (purple), e (red), w (white), y (yellow), l (blue), o (orange), k (black) \\ \hline
does-bruise-or-bleed & categorical & t (bruises or bleeding), f (no) \\ \hline
gill-attachment & categorical & a (adnate), x (adnexed), d (decurrent), e (free), s (sinuate), p (pores), f (none) \\ \hline
gill-spacing & categorical & c (close), d (distant), f (none) \\ \hline
gill-color & categorical & n (brown), b (buff), g (gray), r (green), p (pink), u (purple), e (red), w (white), y (yellow), o (orange), k (black), f (none) \\ \hline
stem-height & numerical & float number in cm \\ \hline
stem-width & numerical & float number in mm \\ \hline
stem-root & categorical & b (bulbous), s (swollen), c (club), r (rooted), f (not specified by the author) \\ \hline
stem-surface & categorical & i (fibrous), y (scaly), s (smooth), h (shiny), k (silky), t (sticky), f (none), g (not specified by the author) \\ \hline
stem-color & categorical & n (brown), b (buff), g (gray), r (green), p (pink), u (purple), e (red), w (white), y (yellow), l (blue), o (orange), k (black), f (none) \\ \hline
veil-type & categorical & u (universal) \\ \hline
veil-color & categorical & n (brown), u (purple), e (red), w (white), y (yellow), k (black) \\ \hline
has-ring & categorical & t (ring), f (none) \\ \hline
ring-type & categorical & e (evanescent), r (flaring), g (grooved), l (large), p (pendant), z (zone), m (movable), f (none) \\ \hline
spore-print-color & categorical & n (brown), g (gray), r (green), p (pink), u (purple), w (white), k (black) \\ \hline
habitat & categorical & g (grasses), l (leaves), m (meadows), p (paths), h (heaths), u (urban), w (waste), d (woods) \\ \hline
season & categorical & s (spring), u (summer), a (autumn), w (winter) \\ \hline
\end{tabular}
\end{table}

The dataset is balanced, with 27,181 mushrooms classified as edible and 33,888 as poisonous, ensuring that no class is overrepresented in subsequent analyses. During preprocessing, 146 duplicate rows were identified and removed to prevent undue weighting of any observation. This step reduced the dataset to 60,923 observations. Additionally, nine variables were identified as containing missing values, as shown in Table \ref{tab:missing_values_before}.

Upon further inspection, a question arose about whether the value ``f'' should be treated as a regular value or as missing. After careful consideration, it was decided to encode all occurrences of ``f'' as missing, except in ``does-bruise-or-bleed'' and ``has-ring'', where ``f'' is interpreted as ``no'' rather than ``none''. This increased the number of variables with missing values to twelve (see Table \ref{tab:missing_values_after}). [write here also about value 0 for numerical]

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Missing Values Count and Percentage}
\label{tab:missing_values_before}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{does-bruise-or-bleed}}
S[table-format=5]
S[table-format=2.2]
}
\hline
& \multicolumn{2}{c}{\textbf{Missing Values}} \\ \cline{2-3}
\textbf{Variable} & \textbf{Count} & \textbf{Percentage} \\ \hline
cap-surface & 14120 & 23.12 \\ \hline
gill-attachment & 9855 & 16.14 \\ \hline
gill-spacing & 25062 & 41.04 \\ \hline
stem-root & 51536 & 84.39 \\ \hline
stem-surface & 38122 & 62.42 \\ \hline
veil-type & 57746 & 94.56 \\ \hline
veil-color & 53510 & 87.62 \\ \hline
ring-type & 2471 & 4.05 \\ \hline
spore-print-color & 54597 & 89.40 \\ \hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Missing Values Count and Percentage (After Replacing 'f' with NaN)}
\label{tab:missing_values_after}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{does-bruise-or-bleed}}
S[table-format=5]
S[table-format=2.2]
}
\hline
& \multicolumn{2}{c}{\textbf{Missing Values}} \\ \cline{2-3}
\textbf{Variable} & \textbf{Count} & \textbf{Percentage} \\ \hline
cap-shape & 13404 & 21.95 \\ \hline
cap-surface & 14120 & 23.12 \\ \hline
gill-attachment & 13269 & 21.73 \\ \hline
gill-spacing & 28476 & 46.63 \\ \hline
gill-color & 3414 & 5.59 \\ \hline
stem-root & 52451 & 85.89 \\ \hline
stem-surface & 39037 & 63.92 \\ \hline
stem-color & 915 & 1.50 \\ \hline
veil-type & 57746 & 94.56 \\ \hline
veil-color & 53510 & 87.62 \\ \hline
ring-type & 50686 & 83.00 \\ \hline
spore-print-color & 54597 & 89.40 \\ \hline
\end{tabular}
\end{table}

It was decided to do the following analyses:\ (1) using the original dataset with missing values (since DecisionTreeClassifier is able to handle missing values), (2) using the original dataset without missing values, (3) using the processed dataset with missing values, (4) using the processed dataset without missing values.

All four analyses were performed using

\end{document}